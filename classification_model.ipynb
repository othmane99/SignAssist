{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2  pour Creer un model de Classification lite pour des Edges Devices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "numbre_of_imgperclass=0\n",
    "\n",
    "dataset_directory=\"C:/Users/oboum/Downloads/archive3/archive/asl_alphabet_train/asl_alphabet_train/\"\n",
    "new_dataset=\"C:/Users/oboum/Downloads/archive3/archive/asl_alphabet_train/new_dataset/\"\n",
    "for dir in os.listdir(dataset_directory):\n",
    "    for img in os.listdir(dataset_directory+dir):\n",
    "      if os.path.exists(new_dataset+dir+\"/\")==False:\n",
    "         os.makedirs(new_dataset+dir+\"/\")\n",
    "      if numbre_of_imgperclass<300:\n",
    "            shutil.copy(dataset_directory+dir+\"/\"+img,new_dataset+dir+\"/\"+img)\n",
    "            numbre_of_imgperclass=numbre_of_imgperclass+1\n",
    "      else:\n",
    "          numbre_of_imgperclass=0\n",
    "          break\n",
    "      #print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 8700, num_label: 29, labels: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, del, nothing, space.\n"
     ]
    }
   ],
   "source": [
    "#image_path=\"C:/Users/oboum/Downloads/archive3/archive/asl_alphabet_train/asl_alphabet_train/\"\n",
    "image_path=new_dataset\n",
    "data = DataLoader.from_folder(image_path)\n",
    "train_data, test_data = data.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hub_keras_layer_v1v2_3 (Hub  (None, 1280)             6992768   \n",
      " KerasLayerV1V2)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 29)                37149     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,029,917\n",
      "Trainable params: 37,149\n",
      "Non-trainable params: 6,992,768\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/17\n",
      "77/77 [==============================] - 239s 3s/step - loss: 2.8575 - accuracy: 0.3258\n",
      "Epoch 2/17\n",
      "77/77 [==============================] - 240s 3s/step - loss: 1.9634 - accuracy: 0.7098\n",
      "Epoch 3/17\n",
      "77/77 [==============================] - 232s 3s/step - loss: 1.5929 - accuracy: 0.8144\n",
      "Epoch 4/17\n",
      "77/77 [==============================] - 232s 3s/step - loss: 1.4053 - accuracy: 0.8644\n",
      "Epoch 5/17\n",
      "77/77 [==============================] - 230s 3s/step - loss: 1.2804 - accuracy: 0.8975\n",
      "Epoch 6/17\n",
      "77/77 [==============================] - 230s 3s/step - loss: 1.2079 - accuracy: 0.9110\n",
      "Epoch 7/17\n",
      "77/77 [==============================] - 228s 3s/step - loss: 1.1485 - accuracy: 0.9323\n",
      "Epoch 8/17\n",
      "77/77 [==============================] - 224s 3s/step - loss: 1.1059 - accuracy: 0.9405\n",
      "Epoch 9/17\n",
      "77/77 [==============================] - 229s 3s/step - loss: 1.0744 - accuracy: 0.9469\n",
      "Epoch 10/17\n",
      "77/77 [==============================] - 227s 3s/step - loss: 1.0473 - accuracy: 0.9527\n",
      "Epoch 11/17\n",
      "77/77 [==============================] - 230s 3s/step - loss: 1.0268 - accuracy: 0.9535\n",
      "Epoch 12/17\n",
      "77/77 [==============================] - 226s 3s/step - loss: 1.0070 - accuracy: 0.9587\n",
      "Epoch 13/17\n",
      "77/77 [==============================] - 235s 3s/step - loss: 0.9915 - accuracy: 0.9622\n",
      "Epoch 14/17\n",
      "77/77 [==============================] - 230s 3s/step - loss: 0.9828 - accuracy: 0.9664\n",
      "Epoch 15/17\n",
      "77/77 [==============================] - 233s 3s/step - loss: 0.9668 - accuracy: 0.9698\n",
      "Epoch 16/17\n",
      "77/77 [==============================] - 243s 3s/step - loss: 0.9626 - accuracy: 0.9688\n",
      "Epoch 17/17\n",
      "77/77 [==============================] - 238s 3s/step - loss: 0.9502 - accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "model = image_classifier.create(train_data,model_spec='efficientnet_lite3',batch_size=90,epochs=17,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.export(export_dir='.')\n",
    "model.export(export_dir='./asl_alphabet_train',label_filename='model300_mobilnetv2.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to tflite model\n",
    "#Our model is all trained up and ready to be used for detecting objects. First, we need to export the model graph (a file that contains information about the architecture and weights) to a TensorFlow Lite-compatible format.\n",
    "tflite_model = tf.keras.models.load_model('SignLanguageALSClassifier.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(tflite_model)\n",
    "tflite = converter.convert()\n",
    "PATH_TO_MODEL='SignLanguageALSClassifier_compressed.tflite'\n",
    "with open(PATH_TO_MODEL, 'wb') as f:\n",
    "  f.write(tflite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I am having one TensorFlow Keras module \"model.h5\". I want to generate tflite from it. I am using the below-mentioned code for that. I am using tensorflow version '2.0.0'.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import lite\n",
    "\n",
    "dataset_dir = \"C:/Users/oboum/Downloads/archive3/archive/asl_alphabet_train/asl_alphabet_train/\"\n",
    "IMAGE_SIZE = 224\n",
    "saved_keras_model = \"'SignLanguageALSClassifier.h5\"\n",
    "\n",
    "def representative_data_gen():\n",
    "  dataset_list = tf.data.Dataset.list_files(dataset_dir + '/*')\n",
    "  for i in range(100):\n",
    "    image = next(iter(dataset_list))\n",
    "    image = tf.io.read_file(image)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    yield [image]\n",
    "\n",
    "converter =  lite.TFLiteConverter.from_keras_model_file(saved_keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# These set the input and output tensors to uint8\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "# And this sets the representative dataset so we can quantize the activations\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('efficient_v0_quant.tflite', 'wb') as f:\n",
    "      f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantize model\n",
    "\"\"\"I'll use the \"TFLiteConverter\" module to perform [post-training quantization] on the model.\n",
    "To quantize the model, we need to provide a representative dataset, which is a set of images that represent what the model will see when deployed in the field. First, we'll create a list of images to include in the representative dataset.\n",
    "\"\"\"\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "PATH_TO_MODEL='mobilenet_model_v3.tflite'\n",
    "interpreter = Interpreter(model_path=PATH_TO_MODEL) \n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "import random\n",
    "\n",
    "quant_image_list=x_train\n",
    "def representative_data_gen():\n",
    "  dataset_list = quant_image_list\n",
    "  quant_num = 300\n",
    "  for i in range(quant_num):\n",
    "    pick_me = random.choice(dataset_list)\n",
    "    image = tf.io.read_file(pick_me)\n",
    "\n",
    "    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n",
    "      image = tf.io.decode_jpeg(image, channels=3)\n",
    "    elif pick_me.endswith('.png'):\n",
    "      image = tf.io.decode_png(image, channels=3)\n",
    "    elif pick_me.endswith('.bmp'):\n",
    "      image = tf.io.decode_bmp(image, channels=3)\n",
    "\n",
    "    image = tf.image.resize(image, [width, height]) \n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    yield [image]\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('saved_model_mobileNet')\n",
    "\n",
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# This enables quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# This sets the representative dataset for quantization\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "# These set the input tensors to uint8 and output tensors to float32\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open('quant_mobilenet_ASLv3.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
